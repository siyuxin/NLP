{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.词频统计的缺陷\n",
    "\n",
    "    我们将下面4个短文本做了词频统计：\n",
    "\n",
    "    corpus=[\"I come to China to travel\", \n",
    "    \"This is a car polupar in China\",          \n",
    "    \"I love tea and Apple \",   \n",
    "    \"The work is to write some papers in science\"] \n",
    "\n",
    "    不考虑停用词，处理后得到的词向量如下：\n",
    "\n",
    "    [[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0]\n",
    "     [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0]\n",
    "     [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]\n",
    "     [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]]\n",
    "\n",
    "    如果我们直接将统计词频后的19维特征做为文本分类的输入，会发现有一些问题。比如第一个文本，我们发现\"come\",\"China\"和“Travel”各出现1次，而“to“出现了两次。似乎看起来这个文本与”to“这个特征更关系紧密。但是实际上”to“是一个非常普遍的词，几乎所有的文本都会用到，因此虽然它的词频为2，但是重要性却比词频为1的\"China\"和“Travel”要低的多。如果我们的向量化特征仅仅用词频表示就无法反应这一点。因此我们需要进一步的预处理来反应文本的这个特征，而这个预处理就是TF-IDF。\n",
    "    \n",
    "2.概述\n",
    "\n",
    "    TF-IDF是Term Frequency -  Inverse Document Frequency的缩写，即“词频-逆文本频率”。它由两部分组成，TF和IDF。\n",
    "\n",
    "    前面的TF也就是我们前面说到的词频，我们之前做的向量化也就是做了文本中各个词的出现频率统计，并作为文本特征，这个很好理解。关键是后面的这个IDF，即“逆文本频率”如何理解。在上一节中，我们讲到几乎所有文本都会出现的\"to\"其词频虽然高，但是重要性却应该比词频低的\"China\"和“Travel”要低。我们的IDF就是来帮助我们来反应这个词的重要性的，进而修正仅仅用词频表示的词特征值。\n",
    "\n",
    "    概括来讲， IDF反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如上文中的“to”。而反过来如果一个词在比较少的文本中出现，那么它的IDF值应该高。比如一些专业的名词如“Machine Learning”。这样的词IDF值应该高。一个极端的情况，如果一个词在所有的文本中都出现，那么它的IDF值应该为0。\n",
    "\n",
    "3.原理\n",
    "\n",
    "     上面是从定性上说明的IDF的作用，那么如何对一个词的IDF进行定量分析呢？这里直接给出一个词x的IDF的基本公式如下：\n",
    "     \n",
    "$$IDF(x)=\\frac{logN}{N(x)}$$\n",
    "\n",
    "    其中，N代表语料库中文本的总数，而N(x)代表语料库中包含词x的文本总数。为什么IDF的基本公式应该是是上面这样的而不是像N/N(x)\n",
    "\n",
    "\n",
    "    上面的IDF公式已经可以使用了，但是在一些特殊的情况会有一些小问题，比如某一个生僻词在语料库中没有，这样我们的分母为0， IDF没有意义了。所以常用的IDF我们需要做一些平滑，使语料库中没有出现的词也可以得到一个合适的IDF值。平滑的方法有很多种，最常见的IDF平滑后的公式之一为：\n",
    "$$IDF(x)=\\frac{logN+1}{N(x)+1}+1$$\n",
    "\n",
    "    有了IDF的定义，我们就可以计算某一个词的TF-IDF值了：\n",
    "$$TF−IDF(x)=TF(x)∗IDF(x)$$\n",
    "\n",
    "    其中TF(x)指词x在当前文本中的词频。\n",
    "4.利用sklearn实现TF-IDF\n",
    "\n",
    "    第一种方法是在用CountVectorizer类向量化之后再调用TfidfTransformer类进行预处理。\n",
    "    第二种方法是直接用TfidfVectorizer完成向量化与TF-IDF预处理。\n",
    "参考：https://www.cnblogs.com/pinard/p/6693230.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16)\t0.4424621378947393\n",
      "  (0, 15)\t0.697684463383976\n",
      "  (0, 4)\t0.4424621378947393\n",
      "  (0, 3)\t0.348842231691988\n",
      "  (1, 14)\t0.45338639737285463\n",
      "  (1, 9)\t0.45338639737285463\n",
      "  (1, 6)\t0.3574550433419527\n",
      "  (1, 5)\t0.3574550433419527\n",
      "  (1, 3)\t0.3574550433419527\n",
      "  (1, 2)\t0.45338639737285463\n",
      "  (2, 12)\t0.5\n",
      "  (2, 7)\t0.5\n",
      "  (2, 1)\t0.5\n",
      "  (2, 0)\t0.5\n",
      "  (3, 18)\t0.3565798233381452\n",
      "  (3, 17)\t0.3565798233381452\n",
      "  (3, 15)\t0.2811316284405006\n",
      "  (3, 13)\t0.3565798233381452\n",
      "  (3, 11)\t0.3565798233381452\n",
      "  (3, 10)\t0.3565798233381452\n",
      "  (3, 8)\t0.3565798233381452\n",
      "  (3, 6)\t0.2811316284405006\n",
      "  (3, 5)\t0.2811316284405006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "\n",
    "corpus=[\"I come to China to travel\", \n",
    "    \"This is a car polupar in China\",          \n",
    "    \"I love tea and Apple \",   \n",
    "    \"The work is to write some papers in science\"] \n",
    "\n",
    "vectorizer=CountVectorizer()\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))  \n",
    "print (tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t0.4424621378947393\n",
      "  (0, 15)\t0.697684463383976\n",
      "  (0, 3)\t0.348842231691988\n",
      "  (0, 16)\t0.4424621378947393\n",
      "  (1, 3)\t0.3574550433419527\n",
      "  (1, 14)\t0.45338639737285463\n",
      "  (1, 6)\t0.3574550433419527\n",
      "  (1, 2)\t0.45338639737285463\n",
      "  (1, 9)\t0.45338639737285463\n",
      "  (1, 5)\t0.3574550433419527\n",
      "  (2, 7)\t0.5\n",
      "  (2, 12)\t0.5\n",
      "  (2, 0)\t0.5\n",
      "  (2, 1)\t0.5\n",
      "  (3, 15)\t0.2811316284405006\n",
      "  (3, 6)\t0.2811316284405006\n",
      "  (3, 5)\t0.2811316284405006\n",
      "  (3, 13)\t0.3565798233381452\n",
      "  (3, 17)\t0.3565798233381452\n",
      "  (3, 18)\t0.3565798233381452\n",
      "  (3, 11)\t0.3565798233381452\n",
      "  (3, 8)\t0.3565798233381452\n",
      "  (3, 10)\t0.3565798233381452\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf2 = TfidfVectorizer()\n",
    "re = tfidf2.fit_transform(corpus)\n",
    "print (re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 互信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.原理\n",
    "\n",
    "    互信息，Mutual Information，缩写为MI，表示两个变量X与Y是否有关系，以及关系的强弱。\n",
    "    如果 (X, Y) ~ p(x, y), X, Y 之间的互信息 I(X; Y)定义为:\n",
    "$$I(X:Y) = \\sum\\limits_{x\\in X} \\sum\\limits_{y\\in Y} p(x,y)log\\frac { p(x,y)}{p(x)p(y)}$$\n",
    "\n",
    "    其衡量的是两个随机变量之间的相关性，即一个随机变量中包含的关于另一个随机变量的信息量。\n",
    "2.PMI\n",
    "\n",
    "        点互信息PMI(Pointwise Mutual Information)这个指标常常用来衡量两个事物之间的相关性（比如两个词）。公式如下：\n",
    "$$PMI(x:y) = log \\frac{p(x,y)}{p(x)p(y)} = log \\frac{p(x|y)}{p(x)} = log \\frac {p(y|x)}{p(y)}$$\n",
    "\n",
    "       如果x跟y不相关，则p(x,y)=p(x)p(y)。二者相关性越大，则p(x, y)就相比于p(x)p(y)越大。\n",
    "\n",
    "       举个自然语言处理中的例子来说，我们想衡量like这个词的极性（正向情感还是负向情感）。我们可以预先挑选一些正向情感的词，比如good。然后计算like跟good的PMI。\n",
    "       \n",
    "       由此可以看出互信息其实就是对X和Y的所有可能的取值情况的点互信息PMI的加权和\n",
    "       \n",
    "3.利用sklearn实现MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6078468394475832\n",
      "0.35810955263431954\n",
      "1.002510220562348\n",
      "0.9952885384500019\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics as mr\n",
    "#  mr.mutual_info_score(label,x)\n",
    "#  label、x为list或array。\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "x = iris.data\n",
    "label = iris.target\n",
    "x0 = x[:, 0]\n",
    "x1 = x[:, 1]\n",
    "x2 = x[:, 2]\n",
    "x3 = x[:, 3]\n",
    "\n",
    "# 计算各特征与label的互信息\n",
    "print(mr.mutual_info_score(x0, label))\n",
    "print(mr.mutual_info_score(x1, label))\n",
    "print(mr.mutual_info_score(x2, label))\n",
    "print(mr.mutual_info_score(x3, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4752453  0.25804218 0.97913995 0.99035641]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    " \n",
    "iris = datasets.load_iris()\n",
    "x = iris.data\n",
    "label = iris.target\n",
    " \n",
    "mutual_info = mutual_info_classif(x, label, discrete_features= False)\n",
    "print(mutual_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "pycharm-3464d5c1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
