{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "### 1.1认知神经学中的注意力\n",
    "   注意力一般分为两种：一种是自上而下的有意识的注意力，称为聚焦式\n",
    "（Focus）注意力。聚焦式注意力是指有预定目的、依赖任务的、主动有意识地\n",
    "聚焦于某一对象的注意力。\n",
    "另一种是自下而上的无意识的注意力，称为基于显著性（Saliency-Based）的注意力。\n",
    "基于显著性的注意力是由外界刺激驱动的注意，不需要主动干预，也和任务无关。\n",
    "\n",
    "一个和注意力有关的例子是鸡尾酒会效应。当一个人在吵闹的鸡尾酒会上\n",
    "和朋友聊天时，尽管周围噪音干扰很多，他还是可以听到朋友的谈话内容，而\n",
    "忽略其他人的声音（聚焦式注意力）。同时，如果未注意到的背景声中有重要的\n",
    "词（比如他的名字），他会马上注意到（显著性注意力）。\n",
    "### 1.2 神经网络中的Attention\n",
    "当用神经网络来处理大量的输入信息时，也可以借鉴人脑的注意力机制，只\n",
    "选择一些关键的信息输入进行处理，来提高神经网络的效率。\n",
    "\n",
    "在目前的神经网 注意力机制也可称为注意力络模型中，我们可以将最大汇聚\n",
    "、门控（Gating）机制来近似 模型。看作是自下而上的基于显著性的注意力机制。\n",
    "\n",
    "除此之外，自上而下的会聚式注意力也是一种有效的信息选择方式。\n",
    "以阅读理解任务为例，给定一篇很长的文章，然后就此文章的内容进行提问。\n",
    "提出的问题只和段落中的一两个句子相关，其余部分都是无关的。\n",
    "\n",
    "为了减小神经网络的计算负担，只需要把相关的片段挑选出来让后续的神经网络来处理，\n",
    "而不需要把所有文章内容都输入给神经网络。\n",
    "\n",
    "用$X = [x_1, · · · , x_N ]$表示N 个输入信息，为了节省计算资源，不需要将所\n",
    "有的N 个输入信息都输入到神经网络进行计算，只需要从X 中选择一些和任务\n",
    "相关的信息输入给神经网络。\n",
    "\n",
    "注意力机制的计算可以分为两步：\n",
    "一是在所有输入信息上计算注意力分布，\n",
    "二是根据注意力分布来计算输入信息的加权平均。\n",
    "\n",
    "#### 注意力分布 \n",
    "给定一个和任务相关的查询向量 q，我们用注意力变量 $z \\in [1, N]$\n",
    "\n",
    "查询向量 q 可以是动态生成 来表示被选择信息的索引位置\n",
    "，即z = i表示选择了第i个输入信息。为了方便计算，\n",
    "\n",
    "采用一种“软性”的信息选择机制，首先计算在给定q 和X 下，选\n",
    "择第i个输入信息的概率$α_i$，\n",
    "$$α_i = p(z = i|X,q)\\\\\n",
    "=softmax(s(x_i,q))$$\n",
    "\n",
    "其中$α_i$ 称为注意力分布（Attention Distribution），$s(x_i\n",
    ", q)$为注意力打分函数，\n",
    "可以使用以下几种方式来计算：\n",
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/20190528125659334.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3MxNDM0MDg4OTU4,size_16,color_FFFFFF,t_70)\n",
    "其中W, U, v为可学习的网络参数，d为输入信息的维度。\n",
    "\n",
    "理论上，加性模型和点积模型的复杂度差不多，\n",
    "但是点积模型在实现上可以更好地利用矩阵乘积，\n",
    "从而计算效率更高。\n",
    "\n",
    "但当输入信息的维度d比较高，点积模型的值通常有比较大方差，\n",
    "从而导致softmax函数的梯度会比较小。\n",
    "因此，缩放点积模型可以较好地 解决这个问题。\n",
    "双线性模型可以看做是一种泛化的点积模型。\n",
    "#### 加权平均\n",
    "注意力分布$α_i$ 可以解释为在给定任务相关的查询q时，第i个信息受\n",
    "关注的程度。采用一种“软性”的信息选择机制对输入信息进行汇总，\n",
    "$$ att(X,q) = \\sum_{i= 1}^Nα_iX_i$$\n",
    "\n",
    "此称为软性注意力机制（Soft Attention Mechanism）。\n",
    "下图给出软性注意力机制的示例。\n",
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/20190528130053308.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3MxNDM0MDg4OTU4,size_16,color_FFFFFF,t_70)\n",
    "### 1.3Attention变体\n",
    "#### 硬性注意力\n",
    "上文提到的注意力是软性注意力，其选择的信息是所有输入信息在注\n",
    "意力分布下的期望。\n",
    "\n",
    "此外，还有一种注意力是只关注到某一个位置上的信息，叫\n",
    "做硬性注意力（Hard Attention）。\n",
    "\n",
    "硬性注意力有两种实现方式：\n",
    "\n",
    "（1）一种是选取最高概率的输入信息，即\n",
    "$$ att(X,q) = x_j,\\quad   j = argmax_{i=1}^N \\alpha_i$$\n",
    "\n",
    "（2）另一种硬性注意力可以通过在注意力分布式上随机采样的方式实现。\n",
    "\n",
    "#### 键值对注意力\n",
    "更一般地，我们可以用键值对（key-value pair）格式来表示输入信息，其\n",
    "中“键”用来计算注意力分布$α_i$，“值”用来计算聚合信息。\n",
    "\n",
    "用$(K, V ) = [(k_1, v_1), · · · ,(k_N , v_N )]$表示N 个输入信息，给定任务相关的\n",
    "查询向量q 时，注意力函数为\n",
    "$$ att((K,V),q) = \\sum_{i=1}^N \\alpha_iv_i$$\n",
    "\n",
    "当K = V 时，键值对模式就等价于普通的注意力机制。\n",
    "\n",
    "## Attention的应用\n",
    "\n",
    "注意力机制主要是用来做信息筛选，从输入信息中选取相关的信息。\n",
    "\n",
    "注意力机制可以分为两步：\n",
    "\n",
    "一是计算注意力分布 α，\n",
    "\n",
    "二是根据 α 来计算输入信息的加权平均。\n",
    "\n",
    "### 2.1指针网络\n",
    "指针网络（Pointer Network）[Vinyals et al., 2015]是一种序列到序列模型，\n",
    "\n",
    "输入是长度为n的向量序列$X = x_1, · · · , x_n$，\n",
    "\n",
    "输出是下标序列$c_{1:m} = c_1, c_2, · · · , c_m，c_i ∈ [1, n], ∀i$。\n",
    "\n",
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/20190528131208257.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3MxNDM0MDg4OTU4,size_16,color_FFFFFF,t_70)\n",
    "### 2.2自注意力模型\n",
    "如果要建立输入序列之间的长距离依赖关系，可以使用以下两种方法：\n",
    "\n",
    "一种方法是增加网络的层数，通过一个深层网络来获取远距离的信息交互\n",
    "\n",
    "另一种方法是使用全连接网络。\n",
    "\n",
    "全连接网络是一种非常直接的建模远距离依赖的模型，\n",
    "但是无法处理变长的输入序列。\n",
    "\n",
    "不同的输入长度，其连接权重的大小也是不同的。\n",
    "\n",
    "这时我们就可以利用注意力机制来“动态”地生成不同连接的权重，\n",
    "这就是自注意力模型（Self-Attention Model）\n",
    "\n",
    "假设输入序列为$X = [x_1, · · · , x_n] \\in R^{d_1 \\times n}$\n",
    "\n",
    "输出序列为$H = [h_1, · · · , h_N ] \\in R^{d_2 \\times N}$\n",
    "\n",
    "首先我们可以通过线性变换得到三组向量序列:\n",
    "\n",
    "$$Q  = W_QX \\in R^{d_3 \\times N}\\\\\n",
    "K  = W_KX \\in R^{d_3 \\times N}\\\\\n",
    "V  = W_VX \\in R^{d_2 \\times N}\n",
    "$$\n",
    "\n",
    "其中Q, K, V 分别为查询向量序列，键向量序列和值向量序列，$W_Q, W_K, W_V$ 分\n",
    "别为可学习的参数矩阵。\n",
    "\n",
    "$$ h_i = att((K,V),q_i) \\\\\n",
    "= \\sum_{j =1} ^ N \\alpha_{ij}v_j\\\\\n",
    "= \\sum_{j =1 }^Nsoftmax(s(k_j,q_i))v_j$$\n",
    "\n",
    "\n",
    "其中$i, j \\in [1, N]$为输出和输入向量序列的位置，连接权重$α_{ij }$由注意力机制动\n",
    "态生成。\n",
    "\n",
    "下图给出全连接模型和自注意力模型的对比，其中实线表示为可学习的权\n",
    "重，虚线表示动态生成的权重。\n",
    "\n",
    "由于自注意力模型的权重是动态生成的，因此可以处理变长的信息序列\n",
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/20190528132202199.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3MxNDM0MDg4OTU4,size_16,color_FFFFFF,t_70)参考 ：https://nndl.github.io/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "pycharm-3464d5c1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
