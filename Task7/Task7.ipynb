{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "#### 1.1 一维卷积 \n",
    "    \n",
    "一维卷积经常用在信号处理中，用于计算信号的延迟累积。\n",
    "\n",
    "假设一个信号发生器每个时刻t产生一个信号xt，其信息的衰减率为wk，即在k − 1个时\n",
    "间步长后，信息为原来的wk 倍。假设w1 = 1, w2 = 1/2, w3 = 1/4，那么在时刻\n",
    "t收到的信号yt 为当前时刻产生的信息和以前时刻延迟信息的叠加，\n",
    "$$ y_t = 1 \\times x_t + 1/2 \\times x_{t-1} + 1/4 \\times x_{t-2}\\\\\n",
    " \\quad = w_1 \\times x_t + w_2 \\times x_{t-1} + w_3 \\times x_{t-2}\\\\\n",
    " \\quad = \\sum_{k =1}^3 w_k \\cdot x_{t-k+1}$$\n",
    "把$w1, w2,...$ 称为滤波器（Filter）或卷积核（Convolution Kernel）。\n",
    "假设滤波器长度为m，它和一个信号序列$x1, x2,...$ 的卷积为\n",
    "$$ y_t  = \\sum_{k =1}^m w_k \\cdot x_{t-k+1}$$\n",
    "![](img/filter_1.png)\n",
    "#### 1.2 二维卷积\n",
    "卷积也经常用在图像处理中。因为图像为一个两维结构，所以需要将\n",
    "一维卷积进行扩展。给定一个图像\n",
    "$X \\in R^{M \\times N}$，和滤波器\n",
    "$W \\in R^{m \\times n}$  一般 $m << M,n<<N$,其卷积为\n",
    "$$ y_{ij} = \\sum_{u = 1}^m \\sum_{v = 1} ^ n w_{uv} \\cdot x_{i-u+1,j-v+1}$$\n",
    "\n",
    "在图像处理中，卷积经常作为特征提取的有效方法。一幅图像在经过卷积\n",
    "操作后得到结果称为特征映射（Feature Map）。图5.3给出在图像处理中几种\n",
    "常用的滤波器，以及其对应的特征映射。图中最上面的滤波器是常用的高斯滤\n",
    "波器，可以用来对图像进行平滑去噪；中间和最下面的过滤器可以用来提取边\n",
    "缘特征。\n",
    "\n",
    "![](img/filter_2.png)\n",
    "#### 1.3 步长和填充\n",
    "滤波器的步长（Stride）是指滤波器在滑动时的时间间隔。\n",
    "\n",
    "零填充（Zero Padding）是在输入向量两端进行补零。\n",
    "![](img/filter_3.png)\n",
    "假设卷积层的输入神经元个数为 (n − m + 2p)/s + 1为整数。 n，卷积大小为m，步长（stride）为s，输\n",
    "入神经元两端各填补 p 个零（zero padding），那么该卷积层的神经元数量为\n",
    "(n − m + 2p)/s + 1。\n",
    "\n",
    "一般常用的卷积有以下三类： \n",
    "\n",
    "• 窄卷积（Narrow Convolution）：步长s = 1，两端不补零p = 0，卷积后\n",
    "输出长度为n − m + 1。\n",
    "\n",
    "• 宽卷积（Wide Convolution）：步长s = 1，两端补零p = m − 1，卷积后\n",
    "输出长度n + m − 1。\n",
    "\n",
    "• 等宽卷积（Equal-Width Convolution）：步长s = 1，两端补零p = (m −\n",
    "1)/2，卷积后输出长度n。\n",
    "### CNN\n",
    "#### 2.1卷积与全连接\n",
    "根据卷积的定义，卷积层有两个很重要的性质：\n",
    "\n",
    "局部连接 在卷积层（假设是第l 层）中的每一个神经元都只和下一层中某个局部窗口内的神经元相连，构成一个局部连接网络。\n",
    "\n",
    "卷积层和下一层之间的连接数大大减少，有原来的$ n^{(l)} \\times n{(l−1)} $个连接变为$n^{(l)} \\times m$\n",
    "个连接，m为滤波器大小。\n",
    "\n",
    "权重共享 ，作为参数的滤波器w(l) 对于第l 层的所有的\n",
    "神经元都是相同的。\n",
    "![](img/fc_vs_cnn.png)\n",
    "#### 2.2 卷积层\n",
    "卷积层的作用是提取一个局部区域的特征，不同的卷积核相当于不同的特\n",
    "征提取器。上一节中描述的卷积层的神经元和全连接网络一样都是一维结构。\n",
    "既然卷积网络主要应用在图像处理上，而图像为两维结构，因此为了更充分地\n",
    "利用图像的局部信息，通常将神经元组织为三维结构的神经层，其大小为高度\n",
    "M×宽度N×深度D，有D 个M × N 大小的特征映射构成。\n",
    "\n",
    "在输入层，特征映射就是图像本身。如果是灰度图像，就是有一个特征映射，深度D = 1；如果是彩色图像，分别有RGB三个颜色通道的特征映射，输入层深度D = 3。\n",
    "\n",
    "不失一般性，假设一个卷积层的结构如下：\n",
    "• 输入特征映射组：$X \\in R^{M×N×D}$ 为三维张量（tensor），其中每个切片\n",
    "(slice)矩阵$X^d \\in  R^{M×N} $ 为一个输入特征映射，$1 ≤ d ≤ D$；\n",
    "\n",
    "• 输出特征映射组：$Y \\in R^{M′×N′×P}$ 为三维张量，其中每个切片矩阵\n",
    "$Y^p \\in R^{M′×N}$ 为一个输出特征映射，$1 ≤ p ≤ P$；\n",
    "\n",
    "• 卷积核：$W \\in R^{m×n×D×P}$ 为四维张量，其中每个切片矩阵$W^{p,d} \\in R^{m×n}$\n",
    "为一个两维卷积核，$1 ≤ d ≤ D, 1 ≤ p ≤ P$。\n",
    "![](img/c_3.png)\n",
    "为了计算输出特征映射$Y^p$,用卷积核$W^{p,1},W^{p,2},...,W^{p,D}$ 分别对输入特\n",
    "征映射$X^1,X^2,...,X^D$ 进行卷积，然后将卷积结果相加，并加上一个标量偏置\n",
    "b得到卷积层的净输入$Z^p$,再经过非线性激活函数后得到输出特征映射$Y^p$\n",
    "$$ Z^p = W^p \\times X +b ^p\\\\\n",
    "=\\sum_{d =1}^D W^{p,d} \\times X^d + b^p$$\n",
    "$$ Y^ p = f(Z^p)$$\n",
    "![](img/feature_map.png)\n",
    "#### 2.3池化层\n",
    "池化层（Pooling Layer）也叫子采样层（Subsampling Layer），其作用是\n",
    "进行特征选择，降低特征数量，并从而减少参数数量。\n",
    "\n",
    "卷积层虽然可以显著减少网络中连接的数量，但特征映射组中的神经元个\n",
    "数并没有显著减少。如果后面接一个分类器，分类器的输入维数依然很高，很\n",
    "容易出现过拟合。为了解决这个问题，可以在卷积层之后加上一个汇聚层，从\n",
    "而降低特征维数，避免过拟合\n",
    "\n",
    "到一个值，作为这个区域的概括。\n",
    "常用的汇聚函数有两种：\n",
    "1. 最大汇聚（Maximum Pooling）：一般是取一个区域内所有神经元的最大值。\n",
    "$$ Y_{m,n}^d = max( x_i), i \\in R^d_{m,n}$$\n",
    "其中$x_i$ 为区域$R^d_{m,n}$ 内每个神经元的激活值。\n",
    "2. 平均汇聚（Mean Pooling）：一般是取区域内所有神经元的平均值。\n",
    "$$ Y_{m,n}^d = \\frac {1} {R^d_{m,n}} \\sum x_i,i \\in R^d_{m,n}$$\n",
    "汇聚层不但\n",
    "可以有效地减少神经元的数量，还可以使得网络对一些小的局部形态改变保持\n",
    "不变性，并拥有更大的感受野。\n",
    "![](img/max_pooling.png)\n",
    "### 典型的网络结构\n",
    "\n",
    "一个典型的卷积网络是由卷积层、汇聚层、全连接层交叉堆叠而成。目前常\n",
    "用的卷积网络结构图5.9所示。一个卷积块为连续M 个卷积层和b个汇聚层（M\n",
    "通常设置为2 ∼ 5，b为0或1）。一个卷积网络中可以堆叠N 个连续的卷积块，\n",
    "然后在接着K 个全连接层（N 的取值区间比较大，比如1 ∼ 100或者更大；K\n",
    "一般为0 ∼ 2）\n",
    "![](img/cnn.png)\n",
    "#### 3.1 LeNet-5\n",
    "![](img/LeNet-5.png)\n",
    "不计输入层，LeNet-5共有7层，每一层的结构为：\n",
    "1. 输入层：输入图像大小为32 × 32 = 1024。\n",
    "2. C1层是卷积层，使用6个5 × 5的滤波器，得到6组大小为28 × 28 = 784\n",
    "    的特征映射。因此，C1层的神经元数量为6 × 784 = 4, 704，可训练参数\n",
    "    数量为6 × 25 + 6 = 156，连接数为156 × 784 = 122, 304（包括偏置在内，\n",
    "    下同）。\n",
    "3. S2层为汇聚层，采样窗口为2×2，使用平均汇聚，并使用一个如公式(5.24)\n",
    "    的非线性函数。神经元个数为 6 × 14 × 14 = 1, 176，可训练参数数量为\n",
    "    6 × (1 + 1) = 12，连接数为6 × 196 × (4 + 1) = 5, 880。\n",
    "4. C3层为卷积层。LeNet-5中用一个连接表来定义输入和输出特征映射之间\n",
    "    的依赖关系，如图5.11所示，共使用60个5 × 5的滤波器，得到16组大小 连接表参见公式(5.37)。\n",
    "    为10 × 10的特征映射。神经元数量为16 × 100 = 1, 600，可训练参数数量 如果不使用连接表，则需要\n",
    "    为(60 × 25) + 16 = 1, 516，连接数为100 × 1, 516 = 151, 600。 96个5 × 5的滤波器。\n",
    "5. S4层是一个汇聚层，采样窗口为2 × 2，得到16个5 × 5大小的特征映射，\n",
    "    可训练参数数量为16 × 2 = 32，连接数为16 × 25 × (4 + 1) = 2000。\n",
    "6. C5层是一个卷积层，使用120 × 16 = 1, 920个5 × 5的滤波器，得到120\n",
    "    组大小为1 × 1的特征映射。C5层的神经元数量为120，可训练参数数量\n",
    "    为1, 920 × 25 + 120 = 48, 120，连接数为120 × (16 × 25 + 1) = 48, 120。\n",
    "7. F6层是一个全连接层，有84个神经元，可训练参数数量为84×(120+1) =\n",
    "    10, 164。连接数和可训练参数个数相同，为10, 164。\n",
    "8. 输出层：输出层由10个欧氏径向基函数（Radial Basis Function，RBF）\n",
    "    函数组成。\n",
    "#### 3.2 AlexNet\n",
    "AlexNet[Krizhevsky et al., 2012]是第一个现代深度卷积网络模型，其首次\n",
    "使用了很多现代深度卷积网络的一些技术方法，比如使用 GPU 进行并行训练，\n",
    "采用了ReLU作为非线性激活函数，使用Dropout 防止过拟合，使用数据增强\n",
    "来提高模型准确率等。AlexNet赢得了2012年ImageNet图像分类竞赛的冠军。\n",
    "\n",
    "AlexNet的结构下图所示，包括5个卷积层、3个全连接层和1个softmax\n",
    "层。因为网络规模超出了当时的单个GPU的内存限制，AlexNet将网络拆为两\n",
    "半，分别放在两个GPU上，GPU间只在某些层（比如第3层）进行通讯。\n",
    "![](img/AlexNet.png)\n",
    "#### 3.3 Inception\n",
    "在卷积网络中，如何设置卷积层的卷积核大小是一个十分关键的问题。在\n",
    "Inception网络中，一个卷积层包含多个不同大小的卷积操作，称为Inception模\n",
    "块。Inception网络是由有多个inception模块和少量的汇聚层堆叠而成。\n",
    "\n",
    "Inception模块同时使用1 × 1、3 × 3、5 × 5等不同大小的卷积核，并将得\n",
    "到的特征映射在深度上拼接（堆叠）起来作为输出特征映射。\n",
    "\n",
    "下图给出了 v1 版本的 Inception 模块，采用了 4 组平行的特征抽取方式，\n",
    "分别为1 × 1、3 × 3、5 × 5的卷积和3 × 3的最大汇聚。同时，为了提高计算效\n",
    "率，减少参数数量，Inception模块在进行3 × 3、5 × 5的卷积之前、3 × 3的最\n",
    "大汇聚之后，进行一次1 × 1的卷积来减少特征映射的深度。如果输入特征映射\n",
    "之间存在冗余信息，1 × 1的卷积相当于先进行一次特征抽取\n",
    "\n",
    "![](img/Inception.png)\n",
    "\n",
    "#### 3.4 ResNet\n",
    "残差网络（Residual Network，ResNet）是通过给非线性的卷积层增加直\n",
    "连边的方式来提高信息的传播效率。\n",
    "\n",
    "残差网络就是将很多个残差单元串联起来构成的一个非常深的网络。\n",
    "\n",
    "下图为一个简单的残差单元\n",
    "![](img/ResNet.png)\n",
    "\n",
    "参考文献 ：\n",
    "\n",
    "[1] : https://nndl.github.io/\n",
    "\n",
    "[2] :Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi.\n",
    "    Inception-v4, inception-resnet and the impact of residual connections on learning. In\n",
    "    AAAI, pages 4278–4284, 2017\n",
    "    \n",
    "[3]:   Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L\n",
    "Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous\n",
    "convolution, and fully connected CRFs.\n",
    "IEEE transactions on pattern analysis and\n",
    "machine intelligence, 40(4):834–848, 2018.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "pycharm-3464d5c1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
