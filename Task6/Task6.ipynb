{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  前馈神经网络 \n",
    "\n",
    "### 神经元\n",
    "![](img/NN.png)\n",
    "\n",
    "假设一个神经元接受d个输入 $X_1,X_2,..,X_d$,令向量 $X = [X_1;X_2;...;X_d]$来\n",
    "表示这组输入，并用净输入（Net Input）$z \\in R $表示一个神经元所获得的输入信号x的加权和，\n",
    "$$ Z = \\sum_{i =1}^d w_ix_i + b \n",
    "\\\\ = W^TX+b$$\n",
    "\n",
    "其中 $ w = [w_1;w_2;...;w_d] \\in R^d$ 是 d维 的权重向量 $ b \\in R$ 是偏置\n",
    "净输入 z 在经过一个非线性函数 $f(x)$ 后，得到神经元的活性值(Activation) a\n",
    "$$ a = f(z) $$\n",
    "其中非线性函数$f(x)$称为激活函数（Activation Function）。\n",
    "### 激活函数 \n",
    "激活函数在神经元中非常重要的。为了增强网络的表示能力和学习能\n",
    "力，激活函数需要具备以下几点性质：\n",
    "1. 连续并可导（允许少数点上不可导）的非线性函数。可导的激活函数可以\n",
    "直接利用数值优化的方法来学习网络参数。\n",
    "2. 激活函数及其导函数要尽可能的简单，有利于提高网络计算效率。\n",
    "3. 激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小，\n",
    "否则会影响训练的效率和稳定性。\n",
    "\n",
    "#### Sigmoid型激活函数\n",
    "Sigmoid型函数是指一类S型曲线函数，为两端饱和函数。常用的Sigmoid\n",
    "型函数有Logistic函数和Tanh函数。\n",
    "Logistic 型 \n",
    "$$f(x) = \\frac {1}{1+exp(-x)}$$\n",
    "Tanh 型\n",
    "$$ tanh(x) = \\frac {exp(x) - exp(-x)} {exp(x) + exp(-x)}$$\n",
    "![](img/sigmoid.png)\n",
    "\n",
    "#### ReLU\n",
    "修正线性单元（Rectified Linear Unit，ReLU）[Nair and Hinton, 2010]，也\n",
    "叫rectifier函数[Glorot et al., 2011]，是目前深层神经网络中经常使用的激活函\n",
    "数。ReLU实际上是一个斜坡（ramp）函数，定义为\n",
    "$$ ReLU(x) = max(0,x)$$\n",
    "带泄露的ReLU（Leaky ReLU）在输入 x < 0时，保持一个很小的梯度$\\lambda$。\n",
    "这样当神经元非激活时也能有一个非零的梯度可以更新参数，避免永远不能被\n",
    "激活[Maas et al., 2013]。带泄露的ReLU的定义如下：\n",
    "$$LeakyReLU(x) = max(x,\\lambda x)$$\n",
    "带参数的ReLU（Parametric ReLU，PReLU）引入一个可学习的参数，不\n",
    "同神经元可以有不同的参数[He et al., 2015]。对于第i个神经元，其PReLU的\n",
    "定义为\n",
    "$$PReLU(x) = max(0,x) + \\lambda_i min(0,x) $$\n",
    "指数线性单元（Exponential Linear Unit，ELU）[Clevert et al., 2015]是一\n",
    "个近似的零中心化的非线性函数，其定义为\n",
    "$$ELU(x) = max(0,x) + min(0,\\lambda(exp(x) -1))$$\n",
    "Softplus函数[Dugas et al., 2001]可以看作是rectifier函数的平滑版本，其\n",
    "定义为\n",
    "$$Softplus(x) = log(1+exp(x))$$\n",
    "![](img/ReLU.png)\n",
    "\n",
    "#### 前馈神经网络 \n",
    "![](img/FN.png)\n",
    "• L：表示神经网络的层数；\n",
    "\n",
    "• $m^{(l)}$：表示第l 层神经元的个数；\n",
    "\n",
    "• $f_l(x)$：表示l 层神经元的激活函数；\n",
    "\n",
    "• $W^{(l)} \\in R^{m^l * m^{l-1}}$：表示l − 1层到第l 层的权重矩阵；\n",
    "\n",
    "• $b^{(l)} \\in R^{m^l}$：表示l − 1层到第l 层的偏置；\n",
    "\n",
    "• $z^{(l)} \\in R^{m^l}$：表示l 层神经元的净输入（净活性值）；\n",
    "\n",
    "• $a^{(l)} \\in R^{m^l}$：表示l 层神经元的输出（活性值）。\n",
    "\n",
    "传播方式\n",
    "$$ z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}$$\n",
    "$$ a^{(l)} = f_l(z^{(l)})$$\n",
    "\n",
    "#### 反向传播\n",
    "![](img/BP.png)\n",
    "#### 优化\n",
    "![](img/batch.png)\n",
    "![](img/learning_rate.png)\n",
    "![](img/GD.png)\n",
    "![](img/center.png)\n",
    "![](img/Mnist.png)\n",
    "#### 参数初始化\n",
    "![](img/Init.png)\n",
    "#### 网络正则化\n",
    "![](img/Regularization1.png)\n",
    "![](img/Regularization2.png)\n",
    "#### 超参数优化\n",
    "![](img/Super.png)\n",
    "#### 归一化\n",
    "![](img/Normalization1.png)\n",
    "![](img/Normalization2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEMO\n",
    "#!/usr/bin/env python \n",
    "# -*- coding:utf-8 -*-\n",
    "import  tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "#定义训练数据batch的大小\n",
    "batch_size = 8\n",
    "\n",
    "#定义参数\n",
    "w1 = tf.Variable(tf.random_normal((2,3),stddev=2,seed = 1))\n",
    "w2 = tf.Variable(tf.random_normal((3,1),stddev=2,seed = 1))\n",
    "\n",
    "x = tf.placeholder(tf.float32,shape = (None,2),name='x-input')\n",
    "y_ =  tf.placeholder(tf.float32,shape = (None,1),name='y-input')\n",
    "\n",
    "#定义前向传播\n",
    "\n",
    "a = tf.matmul(x,w1)\n",
    "y = tf.matmul(a,w2)\n",
    "\n",
    "#定义损失函数 和 反向传播\n",
    "y = tf.sigmoid(y)\n",
    "cross_entropy = -tf.reduce_mean(\n",
    "    y_ * tf.log(tf.clip_by_value(y,1e-10,1.0))\n",
    "    + (1-y_)* tf.log(tf.clip_by_value(1-y,1e-10,1.0))\n",
    ")\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "\n",
    "#随机数生成模拟数据集\n",
    "rdm = RandomState(1)\n",
    "dataset_size = 128\n",
    "X = rdm.rand(dataset_size,2)\n",
    "\n",
    "#定义样本标签 所有x1+x2< 1 的样本都被认为是正样本\n",
    "#其他为负样本  正样本为 1 负样本为 0\n",
    "\n",
    "Y= [[int (x1+x2 <1)]for (x1,x2) in X]\n",
    "\n",
    "#创建Session()\n",
    "with tf.device('/gpu:0'):\n",
    "    with tf.Session() as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        print(sess.run(w1))\n",
    "        print(sess.run(w2))\n",
    "        #设定训练的轮数\n",
    "        STEPS = 5000\n",
    "        for i in range(STEPS ) :\n",
    "            #每次选取 batch 个样本进行训练\n",
    "            start = ( i* batch_size) % dataset_size\n",
    "            end = min(start+batch_size,dataset_size)\n",
    "\n",
    "            #通过选区的样本训练神经网络并更新参数\n",
    "\n",
    "            sess.run(train_step,\n",
    "                     feed_dict = {x:X[start:end],y_:Y[start:end]})\n",
    "            if i % 1000 == 0 :\n",
    "                #每隔一段时间计算在素有数据上的交叉熵并输出\n",
    "                total_cross_entropy = sess.run(\n",
    "                    cross_entropy,feed_dict ={x:X,y_:Y})\n",
    "\n",
    "                print(\"After %d training step(s),cross entropy on all data is %g\"%(i,total_cross_entropy))\n",
    "            print(sess.run(w1))\n",
    "            print(sess.run(w2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext \n",
    "### 架构\n",
    "fastText算法是一种有监督的模型，CBOW，通过上下文预测中间词，而fastText则是通过上下文预测标签（这个标签就是文本的类别，是训练模型之前通过人工标注等方法事先确定下来的）。\n",
    "\n",
    "fastText模型的输入是一个词的序列（一段文本或者一句话)，输出是这个词序列属于不同类别的概率。在序列中的词和词组构成特征向量，特征向量通过线性变换映射到中间层，再由中间层映射到标签。fastText在预测标签时使用了非线性激活函数，但在中间层不使用非线性激活函数。\n",
    "![](img/fasttext.png)\n",
    "第一个权重矩阵w_1可以被视作某个句子的词查找表。词表征被平均成一个文本表征，然后其会被馈送入一个线性分类器。\n",
    "\n",
    "这个构架和CBOW模型相似，只是中间词（middle word）被替换成了标签（label）。该模型将一系列单词作为输入并产生一个预定义类的概率分布。\n",
    "\n",
    "我们使用一个softmax方程来计算这些概率。当数据量巨大时，线性分类器的计算十分昂贵，所以fastText使用了一个基于霍夫曼编码树的分层softmax方法。常用的文本特征表示方法是词袋模型，然而词袋（BoW）中的词顺序是不变的，但是明确考虑该顺序的计算成本通常十分高昂。\n",
    "\n",
    "作为替代，fastText使用n-gram获取额外特征来得到关于局部词顺序的部分信息\n",
    "### 层次Softmax\n",
    "分层softmax的目的是降低softmax层的计算复杂度。 \n",
    "\n",
    "二叉树。Hierarchical softmax本质上是用层级关系替代了扁平化的softmax层，如图1所示，每个叶子节点表示一个词语（即霍夫曼树的结构）\n",
    "\n",
    "我们可以把原来的softmax看做深度为1的树，词表V中的每一个词语表示一个叶子节点。如果把softmax改为二叉树结构，每个word表示叶子节点，那么只需要沿着通向该词语的叶子节点的路径搜索，而不需要考虑其它的节点。这就是为什么fastText可以解决不平衡分类问题，因为在对某个节点进行计算时，完全不依赖于它的上一层的叶子节点（即权重大于它的叶结点），也就是数目较大的label不能影响数目较小的label（即图5中B无法影响A和C）。\n",
    "\n",
    "平衡二叉树的深度是log2(|V|)，因此，最多只需要计算log2(|V|)个节点就能得到目标词语的概率值。hierarchical softmax定义了词表V中所有词语的标准化概率分布。\n",
    "\n",
    "此方法只是加速了训练过程，因为我们可以提前知道将要预测的词语（以及其搜索路径）。在测试过程中，被预测词语是未知的，仍然无法避免计算所有词语的概率值。 \n",
    "### N-gram\n",
    "fastText 还加入了 N-gram 特征。“我爱你”的特征应当是“我”、“爱”、“你”。那么“你爱我”这句话的特征和“我爱你”是一样的，因为“我爱你”的bag（词袋）中也是只包含“你”、“爱”、“我”。\n",
    "\n",
    "还是那句话——“我爱你”：如果使用2-gram，这句话的特征还有 “我-爱”和“爱-你”，这两句话“我爱你”和“你爱我”就能区别开来了，因为“你爱我”的2-gram的特征还包括“你-爱”和“爱-我”，这样就可以区分“你爱我”和“我爱你”了。为了提高效率，实务中会过滤掉低频的 N-gram。否则将会严重影响速度。 \n",
    "\n",
    "在fastText 中一个低维度向量与每个单词都相关。隐藏表征在不同类别所有分类器中进行共享，使得文本信息在不同类别中能够共同使用。这类表征被称为词袋（bag of words）（此处忽视词序）。在 fastText中也使用向量表征单词 n-gram来将局部词序考虑在内，这对很多文本分类问题来说十分重要。\n",
    "\n",
    "参考 ： https://blog.csdn.net/weixin_36604953/article/details/78324834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "classfier = fasttext.supervised(\"E:/NLP/Task6/news_fasttext_train.txt\",\"E:/NLP/Task6/news_fasttext.model\",label_prefix = \"__label__\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "pycharm-3464d5c1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
